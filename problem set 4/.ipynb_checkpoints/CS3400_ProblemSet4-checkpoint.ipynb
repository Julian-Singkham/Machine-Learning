{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Set 4\n",
    "Submitted By: Julian Singkham  \n",
    "Date: 10/16/2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Let’s practice plotting the linear decision boundary learned by a logistic regression model. Assume you are given the following fitted model:\n",
    "$$P(y=1|x)=\\frac{1}{1+e^{-(32-9.4x_1+8x_2)}}$$\n",
    "First, we want to pull out the part of the equation corresponding to the plane and set it equal to 0: \n",
    "$$32-9.4x_1+8x_2=0$$\n",
    "\n",
    "Next, if we re-arrange the equation, we obtain: \n",
    "$$-8x_2=32-9.4x_1$$\n",
    "\n",
    "which simplifies to: \n",
    "$$x_2=-4+1.175x_1$$\n",
    "\n",
    "We can now use NumPy’s linspace function to generate the x1 values and then plug them into the line’s equation to generate the values for x2. Lastly, we can plot the line defined by the two arrays of x1 and x2 values.  \n",
    "\n",
    "Your turn! Find the equation of the line/plane for each fitted model below and plot the line/plane. \n",
    "$$P(y=1|x)=\\frac{1}{1+e^{-(-20.96-123.08x_1+40.46x_2)}}$$\n",
    "$$40.46x_2=20.96+123.08x_1$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'x2')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x1 = np.linspace(0,10,num=10000)\n",
    "x2 = (123.08*x1+20.96)/40.46\n",
    "plt.plot(x1, x2)\n",
    "plt.xlabel(\"X1\")\n",
    "plt.ylabel(\"x2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) A logistic regression model is trained by finding weights which optimize the logistic likelihood function:\n",
    "$$L = \\prod_{i=1}^{N}P(y^i=1|x^i)^{y^i}(1-P(y^i=1|x^i))^{1-y^i}$$\n",
    "\n",
    "where $x^i$ represents the feature vector of the $i^{th}$ sample and $y^i$ represents the label of the $i^{th}$ sample.\n",
    "\n",
    "Logistic regression computes the probability that each sample $x^i$ belongs to class 1. Formally, the\n",
    "logistic regression model is referred to as $P(y^i=1|x^i)$. Since logistic regression only assumes two possible outcomes, the probability that the sample is not in class 1 is given by $1−P(y^i=1|x^i)$. Depending on which class (designated 0 or 1) the sample is in, the exponent of one term will be 1 and the other will be 0. So only one term is used per sample in the calculation of the likelihood.\n",
    "\n",
    "For example, assume we are trying to distinguish between “setosa” and “non-setosa” samples.\n",
    "If a sample i = 5 has relatively small petal lengths and widths, there is a high probability (e.g.,\n",
    "99%) that the sample belongs to the “setosa” class. We would write this as $P(y^5=setosa|x^5)=0.99$. Assume that the sample genuinely does belong to the “setosa” species so $y^5=1$. Since $1-y^5=1-1=0$, the $1-P(y^5=1|x^5)$ is reduced to 1. We then calculate the rest to get the\n",
    "loss for that sample:\n",
    "$$L_i=P(y^i=1|x^i)^{y^i}(1-P(y^i=1|x^i))^{1-y^i}$$\n",
    "$$L_5=0.99^1(1-0.99)^{1-1}$$\n",
    "$$L_5=0.99$$\n",
    "\n",
    "Now, let’s look at the case where the species of sample 7’s species is not setosa but the model\n",
    "predicts that it is. Assume the model gives us a probability of 75% of being in setosa: $P(y^7=setosa|x^7)=0.75$. The true class of the sample is 0 so $y^7=0$.\n",
    "$$L_i=P(y^i=1|x^i)^{y^i}(1-P(y^i=1|x^i))^{1-y^i}$$\n",
    "$$L_7=0.75^0(1-0.75)^{1-0}$$\n",
    "$$L_5=1-0.75$$\n",
    "$$L_5=0.25$$\n",
    "\n",
    "(a) Calculate the likelihood for each sample and overall when all of the model’s estimated probabilities are consistent with the true class.  \n",
    "\n",
    "| P(y=1) | y | Per-Sample Likelihood           |\n",
    "| ------ | - | ------------------------------- |\n",
    "| 0.99   | 1 | $$L=0.99^1(1-0.99)^{1-1}=0.99$$ |\n",
    "| 0.99   | 1 | $$L=0.99^1(1-0.99)^{1-1}=0.99$$ |\n",
    "| 0.01   | 0 | $$L=0.01^0(1-0.01)^{1-0}=0.99$$ |\n",
    "| 0.01   | 0 | $$L=0.01^0(1-0.01)^{1-0}=0.99$$ |\n",
    "| 0.99   | 1 | $$L=0.99^1(1-0.99)^{1-1}=0.99$$ |\n",
    "Overall: 99%\n",
    "\n",
    "\n",
    "(c) Calculate the likelihood for each sample and overall for the following five samples.\n",
    "\n",
    "| P(y=1) | y | Per-Sample Likelihood           |\n",
    "| ------ | - | ------------------------------- |\n",
    "| 0.75   | 1 | $$L=0.75^1(1-0.75)^{1-1}=0.75$$ |\n",
    "| 0.99   | 1 | $$L=0.99^1(1-0.99)^{1-1}=0.99$$ |\n",
    "| 0.25   | 0 | $$L=0.25^0(1-0.25)^{1-0}=0.75$$ |\n",
    "| 0.01   | 0 | $$L=0.01^0(1-0.01)^{1-0}=0.99$$ |\n",
    "| 0.99   | 1 | $$L=0.99^1(1-0.99)^{1-1}=0.99$$ |\n",
    "Overall: 89.4%\n",
    "\n",
    "(d) What is the best possible likelihood value? What is the worst possible likelihood value?  \n",
    "The best possible likelihood is 0.99 as that means there is a near certain possibility that the value is correct/will happen. I don't believe a likelihood of 1.0 is possible. The worst possible likelihood value is 0.0 as that means it will never happen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Interpretability of Logistic Regression  \n",
    "Write question later\n",
    "\n",
    "    1) For instance, if we keep all features constant and we increase x1 by 1 unit, how does y change?\n",
    "y will change by $\\beta_1$\n",
    "\n",
    "    2) TBD\n",
    "        a) If we keep all features constant and we increase x1 by 1 unit, how does the odds change?\n",
    "The odds will change by $e^{-\\beta_1}$\n",
    "\n",
    "3) A logistic regression model was fitted on heart failure clinical records data set (extracted from here). The data consists of clinical features and the target is to predict heart failure (1 means heart failure and 0 means no heart failure). The features and the weights obtained are the following:\n",
    "\n",
    "| Features             | Description                                                   | Weights |\n",
    "| -------------------- | ------------------------------------------------------------- | ------- |\n",
    "| Age                  | Age of the patient (years)                                    | 0.0538  |\n",
    "| Anaemia              | Decrease if red blood cells or hemoglobin (boolean)           | 0.2287  |\n",
    "| Diabetes             | If the patient has diabetes (boolean)                         | 0.1611  |\n",
    "| Ejection Fraction    | Percentage of blood leaving the heart at each contraction (%) | -0.0723 |\n",
    "| High Blood Pressure  | If the patient has hypertension (boolean)                     | 0.3520  |\n",
    "| Creatinine           | Level of serum creatinine in the blood (mg/dL)                | 0.6723  |\n",
    "| Gender               | (1) Woman or (0) Man                                          | -0.23   |\n",
    "\n",
    "    How do you intrepret the weights for the following features: gender and serum creatinine? (How does the odds for heart failure vs no heart failure change if we change each of those features individually?)  \n",
    "\n",
    "Gender wise, woman are 23% less likely to get heart failure. Serum creatinine has a very high factor in determining heart failure as it's weighted at 67.23 percent. This means that an increase in serum creatine levels, greatly increases the likelihood of getting heart failure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Growing Trees - Gini Index\n",
    "There are various algorithms to construct a decision tree. The classification and regression trees (CART) algorithm is the most popular algorithm. It consists of a top-down greedy approach known as recursive binary splitting that aims at dividing the feature space into non-overlapping regions: it begins at the root or top of the tree, finds the best split at the root and then successively repeats the same steps for each child node (top-down: it starts from the root, greedy: it chooses the best split at each step). When constructing a classification tree, the best split is chosen according to a measure that indicates the node impurity - node impurity refers to how much the data in each node or region is mixed. An example of such measure is the Gini index.\n",
    "\n",
    "**Steps of CART algorithm** (classification):  \n",
    "(a) Start at the root  \n",
    "(b) Determine the best feature and threshold to split: for each feature xj , find the best cutpoint or threshold v such that the sum of the node impurities of the two regions (or two nodes) defined by $x_j < v$ and $x_j ≥ v$ is minimized. Then choose the best feature.  \n",
    "(c) If a stopping criterion is reached, exit. Otherwise, repeat for each child node.  \n",
    "\n",
    "In this exercise, you will explore the Gini index and work on finding the root node of a small\n",
    "classification dataset.   \n",
    "\n",
    "1. When a split is performed while constructing a classification tree, two regions in the feature space are created, where each region contains a subset of the training data. To measure how much this subset is impure (or mixed) in a given region, we find the Gini index. If the data consists of C classes, the Gini index (G) is defined as:\n",
    "\n",
    "(a) Find the Gini index for each of the following three cases:\n",
    "* the data points in a region all belong to class 1  \n",
    "G = 1\n",
    "\n",
    "* the data points in a region all belong to class 0  \n",
    "G = 0\n",
    "\n",
    "* half of the data points in a region belong to class 1  \n",
    "G = 0.50\n",
    "\n",
    "(b) Plot the function of Gini index with respect to prop(of class 1).\n",
    "\n",
    "2. Consider the following training dataset:  \n",
    "(a)Find the Gini index for the overall training dataset.  \n",
    "$$G=2*(0.6)(1-0.6)=0.48$$  \n",
    "\n",
    "(d) Which feature should be chosen for the root node?    \n",
    "prevalentHyp should be used as it better divides the tree evenly  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
